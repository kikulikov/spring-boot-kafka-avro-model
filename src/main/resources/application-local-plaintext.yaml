application.topic.books: plaintext-books
application.topic.orders: plaintext-orders
application.topic.counts: plaintext-counts

---
spring:
  config:
    activate:
      on-profile: local-plaintext
  kafka:
    properties:
      bootstrap.servers: localhost:9092
      schema.registry.url: http://localhost:8081
    producer:
      client-id: local-plaintext-avro-producer
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: io.confluent.kafka.serializers.KafkaAvroSerializer
    consumer:
      auto-offset-reset: latest
      group-id: local-plaintext-avro-consumer
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: io.confluent.kafka.serializers.KafkaAvroDeserializer
      # By default, KafkaAvroDeserializerConfig.SPECIFIC_AVRO_READER_CONFIG is set to false,
      # so your KafkaAvroDeserializer will produce a GenericData by default, and not SpecificData (POJO).
      properties:
        specific.avro.reader: true
    streams:
      application-id: local-plaintext-avro-streams
      replication-factor: 1
      properties:
        commit.interval.ms: 100
        default.key.serde: org.apache.kafka.common.serialization.Serdes$StringSerde
        default.value.serde: org.apache.kafka.common.serialization.Serdes$StringSerde
        default.deserialization.exception.handler: org.apache.kafka.streams.errors.LogAndFailExceptionHandler
#spring.kafka.streams.properties.schema.registry.url=http://localhost:8081
#spring.kafka.streams.properties.state.dir=/var/lib/kafka-streams
#spring.kafka.streams.properties.state.cleanup.delay.ms: 600000
#spring.kafka.streams.properties.timestamp.extractor: org.apache.kafka.streams.processor.WallclockTimestampExtractor


